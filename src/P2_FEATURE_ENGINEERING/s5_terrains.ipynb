{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terrain Prices\n",
    "\n",
    "@roman \n",
    "\n",
    "5 Nov, 2024\n",
    "\n",
    "Estimate the value of the land in space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import h3\n",
    "from shapely.geometry import Polygon\n",
    "from tqdm import tqdm\n",
    "\n",
    "from INEGIpy import MarcoGeoestadistico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "pd.options.display.max_columns = 120\n",
    "geo_framework = MarcoGeoestadistico()\n",
    "\n",
    "# params\n",
    "HEX_RESOLUTION = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# properties\n",
    "gdf_properties = pd.read_parquet('../../data/interim/cleaned_data_s5.parquet')\n",
    "\n",
    "# to geodataframe\n",
    "gdf_properties = gpd.GeoDataFrame(\n",
    "    gdf_properties,\n",
    "    geometry=gpd.points_from_xy(gdf_properties['longitude'], gdf_properties['latitude']),\n",
    "    crs='EPSG:4326'\n",
    "    )\n",
    "\n",
    "# add land_price_per_sqm\n",
    "gdf_properties['land_price_per_sqm'] = gdf_properties['land_price'] / gdf_properties['land_area']\n",
    "\n",
    "# get only importnat columns\n",
    "cols_to_stay = [\n",
    "    'observation_id', 'valuation_date', 'property_class_id',\n",
    "    'land_price_per_sqm', 'city_cluster', 'latitude', 'longitude'\n",
    "]\n",
    "\n",
    "# subset to important columns\n",
    "gdf_properties = gdf_properties[cols_to_stay]\n",
    "\n",
    "\n",
    "# see\n",
    "print(gdf_properties.shape)\n",
    "gdf_properties.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cities\n",
    "gdf_cities = gpd.read_parquet('../../data/misc/cities.parquet')\n",
    "\n",
    "# crs to 4326\n",
    "gdf_cities = gdf_cities.to_crs(epsg=4326)\n",
    "\n",
    "# rename cluster to city_cluster\n",
    "gdf_cities.rename(columns={'cluster': 'city_cluster'}, inplace=True)\n",
    "\n",
    "# see\n",
    "print(gdf_cities.shape)\n",
    "gdf_cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mexico shape\n",
    "gdf_mex_states = geo_framework.Entidades()\n",
    "gdf_mex_states.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cities\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "gdf_mex_states.plot(ax=ax, color='white', edgecolor='gray')\n",
    "gdf_cities.plot(ax=ax, color='C0', edgecolor='black')\n",
    "\n",
    "# eliminate ticks\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we use lat/lon coordinates\n",
    "def to_latlon(gdf):\n",
    "    if gdf.crs is not None and gdf.crs.to_string() != 'EPSG:4326':\n",
    "        gdf = gdf.to_crs('EPSG:4326')\n",
    "    return gdf\n",
    "\n",
    "\n",
    "# Get all H3 hexagons covering a shapely geometry\n",
    "def h3_polyfill(geometry, resolution):\n",
    "    hexagons = set()\n",
    "    if geometry.geom_type == 'Polygon':\n",
    "        hexagons.update(h3.polyfill(geometry.__geo_interface__, resolution, geo_json_conformant=True))\n",
    "    elif geometry.geom_type == 'MultiPolygon':\n",
    "        for poly in geometry:\n",
    "            hexagons.update(h3.polyfill(poly.__geo_interface__, resolution, geo_json_conformant=True))\n",
    "    return hexagons\n",
    "\n",
    "\n",
    "# Create GeoDataFrame of H3 hexagons from a set of H3 indices\n",
    "def hexagons_to_geodf(hexagons, crs):\n",
    "    # each hexagon to polygon\n",
    "    polygons = [Polygon(h3.h3_to_geo_boundary(h, geo_json=True)) for h in hexagons]\n",
    "    # create geodataframe\n",
    "    gdf = gpd.GeoDataFrame(geometry=polygons, crs=crs)\n",
    "    return gdf\n",
    "\n",
    "\n",
    "# Vectorized function to compute hexagons\n",
    "def create_hex_grid_vectorized(gdf, resolution):\n",
    "    # copy\n",
    "    gdf = gdf.copy()\n",
    "\n",
    "    # buffer by 1km the geometries\n",
    "    gdf = gdf.to_crs('EPSG:6372')\n",
    "    gdf['geometry'] = gdf['geometry'].buffer(1_000)\n",
    "\n",
    "    # Convert GeoDataFrame to lat/lon if not already\n",
    "    gdf = to_latlon(gdf)\n",
    "\n",
    "    # Get H3 hexagons for all geometries\n",
    "    hex_sets = np.array([h3_polyfill(geom, resolution) for geom in gdf['geometry']])\n",
    "    \n",
    "    # Flatten the hexagon sets\n",
    "    all_hexagons = set().union(*hex_sets)\n",
    "\n",
    "    # Convert H3 indices to GeoDataFrame\n",
    "    hex_gdf = hexagons_to_geodf(all_hexagons, gdf.crs)\n",
    "\n",
    "    # Add h3 index\n",
    "    hex_gdf['hex_id'] = hex_gdf.apply(\n",
    "        lambda x: h3.geo_to_h3(x.geometry.centroid.y, x.geometry.centroid.x, resolution),\n",
    "        axis=1\n",
    "        )\n",
    "    \n",
    "    # Merge with gdf to get the city\n",
    "    hex_gdf = (\n",
    "        hex_gdf\n",
    "        .sjoin(\n",
    "            gdf[['city_cluster', 'geometry']], how='left', predicate='intersects'\n",
    "        )\n",
    "        # drop duplicates\n",
    "        .drop_duplicates(subset='hex_id')\n",
    "        .drop(columns='index_right')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return hex_gdf\n",
    "\n",
    "\n",
    "# Create the hexagon grid\n",
    "hex_gdf = create_hex_grid_vectorized(gdf_cities, HEX_RESOLUTION)\n",
    "\n",
    "# See\n",
    "print(hex_gdf.shape)\n",
    "hex_gdf.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latlon_to_h3(latitudes, longitudes, resolution):\n",
    "    \"\"\"\n",
    "    Convert latitude and longitude arrays to H3 hexagon IDs at a given resolution.\n",
    "    \n",
    "    Parameters:\n",
    "    - latitudes: numpy array of latitudes\n",
    "    - longitudes: numpy array of longitudes\n",
    "    - resolution: H3 resolution\n",
    "    \n",
    "    Returns:\n",
    "    - numpy array of H3 hexagon IDs\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays\n",
    "    latitudes = np.asarray(latitudes)\n",
    "    longitudes = np.asarray(longitudes)\n",
    "    \n",
    "    # Check that latitudes and longitudes are the same shape\n",
    "    if latitudes.shape != longitudes.shape:\n",
    "        raise ValueError(\"Latitudes and longitudes must be the same shape\")\n",
    "    \n",
    "    # Vectorize the h3 function\n",
    "    vectorized_h3 = np.vectorize(h3.geo_to_h3)\n",
    "    \n",
    "    # Apply the function to the latitude and longitude arrays\n",
    "    hex_ids = vectorized_h3(latitudes, longitudes, resolution)\n",
    "    \n",
    "    return hex_ids\n",
    "\n",
    "# Get hex id for each property\n",
    "gdf_properties['hex_id'] = latlon_to_h3(\n",
    "    gdf_properties['latitude'].values,\n",
    "    gdf_properties['longitude'].values,\n",
    "    HEX_RESOLUTION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log terrain value\n",
    "gdf_properties['log_valor_fisico_terreno_m2'] = np.log(gdf_properties['land_price_per_sqm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count # of hexagons per city\n",
    "hex_gdf['city_cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look nans\n",
    "hex_gdf['city_cluster'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# KRing Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S1: Unpooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate mean & std of terrain value by hexagon\n",
    "gdf_unpooled = (\n",
    "    gdf_properties\n",
    "    .groupby(['city_cluster', 'hex_id'], as_index=False)\n",
    "    .agg(\n",
    "        mean_log_valor_fisico_terreno_m2=('log_valor_fisico_terreno_m2', 'mean'),\n",
    "        std_log_valor_fisico_terreno_m2=('log_valor_fisico_terreno_m2', 'std'),\n",
    "        num_properties=('observation_id', 'count')\n",
    "    )\n",
    ")\n",
    "\n",
    "# see\n",
    "print(gdf_unpooled.shape)\n",
    "gdf_unpooled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mean, std and counts\n",
    "def plot_hexagons(gdf, column, cluster, cmap='viridis'):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "    \n",
    "    # plot\n",
    "    (\n",
    "        gdf\n",
    "        .query('city_cluster == @cluster')\n",
    "        .merge(hex_gdf[['hex_id', 'geometry']], on='hex_id', how='left')\n",
    "        .pipe(gpd.GeoDataFrame, crs='EPSG:4326', geometry='geometry')\n",
    "        .plot(\n",
    "            ax=ax,\n",
    "            column=column,\n",
    "            cmap=cmap,\n",
    "            legend=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # add title\n",
    "    ax.set_title(f\"{column} - Cluster {cluster}\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# plot mean, std and counts\n",
    "cols_to_plot = [\n",
    "    'mean_log_valor_fisico_terreno_m2',\n",
    "    'std_log_valor_fisico_terreno_m2',\n",
    "    'num_properties'\n",
    "]\n",
    "map_cmap = {\n",
    "    'mean_log_valor_fisico_terreno_m2': 'viridis',\n",
    "    'std_log_valor_fisico_terreno_m2': 'magma',\n",
    "    'num_properties': 'bone'\n",
    "}\n",
    "cluster_name = 10\n",
    "\n",
    "for col in cols_to_plot:\n",
    "    plot_hexagons(\n",
    "        gdf_unpooled,\n",
    "        col,\n",
    "        cluster=cluster_name,\n",
    "        cmap=map_cmap[col]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2: Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted mean & std\n",
    "def mle_weighted_stats(m, s, w, n):\n",
    "    \"\"\"\n",
    "    Compute the weighted mean and standard deviation using the MLE formulas for the normal distribution.\n",
    "\n",
    "    m: mean\n",
    "    s: standard deviation\n",
    "    w: weights\n",
    "    n: number of observations\n",
    "    \"\"\"\n",
    "    # get indexes where s > 0 \n",
    "    idxs = s > 0\n",
    "\n",
    "    # estimates\n",
    "    s2 = s**2\n",
    "    mle_weights = w * (n / s2)\n",
    "\n",
    "    # pooled mean\n",
    "    # p_mean =(\n",
    "    #     np.sum(mle_weights[idxs] * m[idxs])\n",
    "    #     + np.sum(w[~idxs] * m[~idxs])\n",
    "    # )\n",
    "    # p_mean /= (np.sum(mle_weights[idxs]) + np.sum(w[~idxs] + n[~idxs]))\n",
    "\n",
    "    # weights = w * np.log(n)\n",
    "    weights = w * n\n",
    "    p_mean = np.sum(weights * m) / np.sum(weights)  # TODO: change estimator for one that includes sigma!\n",
    "\n",
    "    # pooled std\n",
    "    p_var = (\n",
    "        np.sum(w[idxs] * ((n[idxs] - 1) * s2[idxs] + n[idxs] * (m[idxs] - p_mean)**2))\n",
    "        + np.sum(w[~idxs] * n[~idxs] * (m[~idxs] - p_mean)**2)\n",
    "    )\n",
    "    p_var /= np.sum(w * n)\n",
    "\n",
    "    return pd.Series({'w_mean': p_mean, 'w_std': np.sqrt(p_var), 'num_properties': np.sum(n)})\n",
    "\n",
    "\n",
    "def get_h3_distance(hex_id, hex_ring, meters_factor):\n",
    "    \"\"\"\n",
    "    Get the distance in meters between a hexagon and a ring of hexagons.\n",
    "    \"\"\"\n",
    "    return h3.h3_distance(hex_id, hex_ring) * meters_factor\n",
    "\n",
    "\n",
    "def get_distance_weight(d, xi=0.1):\n",
    "    \"\"\"\n",
    "    Get the weight of a distance using an exponential decayment by 100m\n",
    "    \"\"\"\n",
    "    return np.exp(-xi * d / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kring_smoothing(df_obj, df_data, city_cluster, metric_col='log_valor_fisico_terreno_m2', k_rings=3, distance_decayment=1):\n",
    "    \"\"\"\n",
    "    Performs k-ring smoothing on the provided dataframes.\n",
    "\n",
    "    Parameters:\n",
    "        df_obj (DataFrame): DataFrame containing hexagon IDs and other related data.\n",
    "        df_data (DataFrame): DataFrame containing data to be smoothed.\n",
    "        city_cluster (int): City cluster value to filter the data.\n",
    "        metric_col (str): Column name to be used for smoothing.\n",
    "        k_rings (int): Number of k-rings for smoothing.\n",
    "        distance_decayment (float): Decayment factor for distance weighting.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Smoothed DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Subset the dataframes based on city_cluster\n",
    "    df_obj = df_obj.query(f\"city_cluster == {city_cluster}\").reset_index(drop=True)\n",
    "    df_data = df_data.query(f\"city_cluster == {city_cluster}\").reset_index(drop=True)\n",
    "\n",
    "    # Define parameters\n",
    "    distance_resolution_h3 = h3.edge_length(HEX_RESOLUTION, 'm')\n",
    "\n",
    "    # Step 1: Generate DataFrame of only hex_ids\n",
    "    df_k = df_obj[['hex_id']]\n",
    "    df_k.index = df_k['hex_id']\n",
    "\n",
    "    # Step 2: Explode k-rings\n",
    "    df_k = (df_k.loc[:, 'hex_id']\n",
    "            .apply(lambda x: pd.Series(list(h3.k_ring(x, k_rings))))\n",
    "            .stack()\n",
    "            .to_frame('hex_k')\n",
    "            .reset_index(1, drop=True)\n",
    "            .reset_index())\n",
    "\n",
    "    # Step 3: Merge with df_data\n",
    "    df_k = (df_k.merge(df_data, left_on='hex_k', right_on='hex_id', how='left', suffixes=('', '_k'))\n",
    "                .drop(columns='hex_id_k'))\n",
    "\n",
    "    # Step 4: Get distance in meters between hex_id and hex_k\n",
    "    df_k['distance_m'] = df_k.apply(\n",
    "        lambda x: get_h3_distance(x['hex_id'], x['hex_k'], meters_factor=distance_resolution_h3), axis=1\n",
    "    )\n",
    "\n",
    "    # Step 5: Get distance weight\n",
    "    df_k = df_k.assign(\n",
    "        unorm_weight=lambda x: x['distance_m'].apply(lambda x: get_distance_weight(x, xi=distance_decayment)),\n",
    "        unorm_weight_nan=lambda x: np.where(x['num_properties'].gt(0), x['unorm_weight'], np.nan),\n",
    "        weight=lambda x: x['unorm_weight_nan'] / x.groupby('hex_id')['unorm_weight_nan'].transform('sum')\n",
    "    )\n",
    "\n",
    "    # Step 6: Get weighted statistics\n",
    "    df_summary = (df_k.groupby('hex_id', as_index=False)\n",
    "                      .apply(lambda x: mle_weighted_stats(\n",
    "                          m=x[f\"mean_{metric_col}\"],\n",
    "                          s=x[f\"std_{metric_col}\"],\n",
    "                          w=x['weight'],\n",
    "                          n=x['num_properties']\n",
    "                      ), include_groups=False))\n",
    "\n",
    "    return df_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do kring smoothing for each cluster\n",
    "cluster_list = gdf_unpooled['city_cluster'].unique()\n",
    "\n",
    "# params\n",
    "k_rings = 4\n",
    "distance_decayment = 1\n",
    "\n",
    "# smoothing\n",
    "list_smoothed = []\n",
    "\n",
    "for cluster in tqdm(cluster_list):\n",
    "    smoothed = kring_smoothing(\n",
    "        df_obj=hex_gdf,\n",
    "        df_data=gdf_unpooled,\n",
    "        city_cluster=cluster,\n",
    "        metric_col='log_valor_fisico_terreno_m2',\n",
    "        k_rings=k_rings,\n",
    "        distance_decayment=distance_decayment\n",
    "    )\n",
    "    smoothed['city_cluster'] = cluster\n",
    "    list_smoothed.append(smoothed)\n",
    "\n",
    "# concat\n",
    "gdf_smoothed = pd.concat(list_smoothed, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3: Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see\n",
    "print(gdf_smoothed.shape)\n",
    "gdf_smoothed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "cols_to_plot = [\n",
    "    'w_mean',\n",
    "    'w_std',\n",
    "    'num_properties'\n",
    "]\n",
    "map_cmap = {\n",
    "    'w_mean': 'viridis',\n",
    "    'w_std': 'magma',\n",
    "    'num_properties': 'bone'\n",
    "}\n",
    "\n",
    "for col in cols_to_plot:\n",
    "    plot_hexagons(\n",
    "        gdf_smoothed,\n",
    "        col,\n",
    "        cluster=cluster_name,\n",
    "        cmap=map_cmap[col]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot hexagons that didt have values\n",
    "plot_hexagons(\n",
    "    gdf_smoothed.assign(had_values=lambda x: x['num_properties'].gt(0)),\n",
    "    'had_values',\n",
    "    cluster=cluster_name,\n",
    "    cmap='coolwarm'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how many hexagons had values\n",
    "print(gdf_smoothed['num_properties'].gt(0).sum())\n",
    "print(gdf_smoothed['num_properties'].gt(0).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S1: Expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand mean & std\n",
    "def expand_stats(m, s, w, n, lamb, m_min, s_max, punishment=0.5):\n",
    "    \"\"\"\n",
    "    Expand the mean and standard deviation of a distribution by a factor lambda.\n",
    "\n",
    "    m: mean\n",
    "    s: standard deviation\n",
    "    w: weights\n",
    "    lamb: factor to expand\n",
    "    m_min: minimum mean\n",
    "    s_max: maximum standard deviation\n",
    "    punishment: factor to punish\n",
    "    \"\"\"\n",
    "    # find how many info there is\n",
    "    sum_lambda = np.sum(lamb)\n",
    "    if sum_lambda == 0:\n",
    "        return pd.Series({\n",
    "            'w_mean': m_min * punishment,\n",
    "            'w_std': s_max * punishment,\n",
    "            'num_properties': 0\n",
    "            })\n",
    "    \n",
    "    # estimates\n",
    "    series_estimates = mle_weighted_stats(m, s, w, n)\n",
    "    # expand\n",
    "    series_estimates['w_mean'] = sum_lambda * series_estimates['w_mean'] + (1 - sum_lambda) * (m_min * punishment)\n",
    "    series_estimates['w_std'] = sum_lambda * series_estimates['w_std'] + (1 - sum_lambda) * (s_max * punishment)\n",
    "\n",
    "    # return\n",
    "    return series_estimates\n",
    "\n",
    "\n",
    "def expand_stats(m, s, w, n, lamb, m_min, s_max, punishment=0.5):\n",
    "    \"\"\"\n",
    "    Expand the mean and standard deviation of a distribution by a factor lambda.\n",
    "\n",
    "    Parameters:\n",
    "        m (Series): Mean values.\n",
    "        s (Series): Standard deviation values.\n",
    "        w (Series): Weights.\n",
    "        n (Series): Number of properties.\n",
    "        lamb (Series): Lambda factor to expand.\n",
    "        m_min (float): Minimum mean.\n",
    "        s_max (float): Maximum standard deviation.\n",
    "        punishment (float): Factor to punish.\n",
    "\n",
    "    Returns:\n",
    "        Series: Expanded statistics.\n",
    "    \"\"\"\n",
    "    sum_lambda = np.sqrt(np.sum(lamb))\n",
    "    if sum_lambda == 0:\n",
    "        return pd.Series({\n",
    "            'w_mean': m_min * punishment,\n",
    "            'w_std': s_max * punishment,\n",
    "            'num_properties': 0\n",
    "        })\n",
    "\n",
    "    series_estimates = mle_weighted_stats(m, s, w, n)\n",
    "    series_estimates['w_mean'] = sum_lambda * series_estimates['w_mean'] + (1 - sum_lambda) * (m_min * punishment)\n",
    "    series_estimates['w_std'] = sum_lambda * series_estimates['w_std'] + (1 - sum_lambda) * (s_max * punishment)\n",
    "\n",
    "    return series_estimates\n",
    "\n",
    "\n",
    "def kring_expand(df_obj, df_data, metric_col='log_valor_fisico_terreno_m2', k_rings=15, distance_decayment=0.1, percent_punishment=0.8):\n",
    "    \"\"\"\n",
    "    Performs k-ring smoothing on the provided dataframes and expands statistics.\n",
    "\n",
    "    Parameters:\n",
    "        df_obj (DataFrame): DataFrame containing hexagon IDs and other related data with no properties.\n",
    "        df_data (DataFrame): DataFrame containing data to be smoothed with properties.\n",
    "        metric_col (str): Column name to be used for smoothing.\n",
    "        k_rings (int): Number of k-rings for smoothing.\n",
    "        distance_decayment (float): Decayment factor for distance weighting.\n",
    "        percent_punishment (float): Punishment factor for expanding statistics.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Smoothed and expanded DataFrame.\n",
    "    \"\"\"\n",
    "    # copy\n",
    "    df_obj = df_obj.copy()\n",
    "    df_data = df_data.copy()\n",
    "\n",
    "    # Define parameters\n",
    "    distance_resolution_h3 = h3.edge_length(HEX_RESOLUTION, 'm')\n",
    "\n",
    "    # Step 1: Generate DataFrame of only hex_ids\n",
    "    df_k = df_obj[['hex_id']]\n",
    "    df_k.index = df_k['hex_id']\n",
    "\n",
    "    # Step 2: Explode k-rings\n",
    "    df_k = (df_k.loc[:, 'hex_id']\n",
    "            .apply(lambda x: pd.Series(list(h3.k_ring(x, k_rings))))\n",
    "            .stack()\n",
    "            .to_frame('hex_k')\n",
    "            .reset_index(1, drop=True)\n",
    "            .reset_index())\n",
    "\n",
    "    # Step 3: Merge with df_data\n",
    "    df_k = (df_k.merge(df_data, left_on='hex_k', right_on='hex_id', how='left', suffixes=('', '_k'))\n",
    "                .drop(columns='hex_id_k'))\n",
    "\n",
    "    # Step 4: Get distance in meters between hex_id and hex_k\n",
    "    df_k['distance_m'] = df_k.apply(\n",
    "        lambda x: get_h3_distance(x['hex_id'], x['hex_k'], meters_factor=distance_resolution_h3), axis=1\n",
    "    )\n",
    "\n",
    "    # Step 5: Get distance weight\n",
    "    df_k = df_k.assign(\n",
    "        unorm_weight=lambda x: x['distance_m'].apply(lambda x: get_distance_weight(x, xi=distance_decayment)),\n",
    "        unorm_weight_nan=lambda x: np.where(x['num_properties'].gt(0), x['unorm_weight'], np.nan),\n",
    "        weight=lambda x: x['unorm_weight_nan'] / x.groupby('hex_id')['unorm_weight_nan'].transform('sum'),\n",
    "        lamb_raw=lambda x: x['unorm_weight'] / x.groupby('hex_id')['unorm_weight'].transform('sum'),\n",
    "        lamb=lambda x: np.where(x['num_properties'].gt(0), x['lamb_raw'], 0)\n",
    "    )\n",
    "\n",
    "    # Step 6: Min wmean and max wstd\n",
    "    min_wmean = df_data['w_mean'].min()\n",
    "    max_wstd = df_data['w_std'].max()\n",
    "\n",
    "    # Step 7: Get weighted statistics\n",
    "    df_summary_expanded = (df_k.groupby('hex_id', as_index=False)\n",
    "                           .apply(lambda x: expand_stats(\n",
    "                               m=x['w_mean'],\n",
    "                               s=x['w_std'],\n",
    "                               w=x['weight'],\n",
    "                               n=x['num_properties'],\n",
    "                               lamb=x['lamb'],\n",
    "                               m_min=min_wmean,\n",
    "                               s_max=max_wstd,\n",
    "                               punishment=percent_punishment\n",
    "                           ), include_groups=False))\n",
    "\n",
    "    return df_summary_expanded\n",
    "\n",
    "\n",
    "def subset_data(df, cluster):\n",
    "    \"\"\"\n",
    "    Subset data by cluster.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df\n",
    "        .query(f\"city_cluster == {cluster}\")\n",
    "        .assign(num_properties=lambda x: np.sqrt(x['num_properties']))\n",
    "        .reset_index(drop=True)\n",
    "        .copy()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset hex with no values\n",
    "gdf_smoothed_no_values = gdf_smoothed.query('num_properties == 0').reset_index(drop=True)\n",
    "gdf_smoothed_with_values = gdf_smoothed.query('num_properties > 0').reset_index(drop=True)\n",
    "print(f\"Hex to expand: {gdf_smoothed_no_values.shape[0]}\")\n",
    "\n",
    "# do kring expand for hexagons with no values\n",
    "cluster_list = gdf_smoothed_no_values['city_cluster'].unique()\n",
    "\n",
    "# params\n",
    "k_rings = 10\n",
    "distance_decayment = 0.1\n",
    "\n",
    "# expand\n",
    "list_expanded = []\n",
    "\n",
    "for cluster in tqdm(cluster_list):\n",
    "    # subset\n",
    "    temp_gdf_smoothed_no_values = subset_data(gdf_smoothed_no_values, cluster)\n",
    "    temp_gdf_smoothed_with_values = subset_data(gdf_smoothed_with_values, cluster)\n",
    "\n",
    "    # expand\n",
    "    expanded = kring_expand(\n",
    "        df_obj=temp_gdf_smoothed_no_values,\n",
    "        df_data=temp_gdf_smoothed_with_values,\n",
    "        metric_col='log_valor_fisico_terreno_m2',\n",
    "        k_rings=k_rings,\n",
    "        distance_decayment=distance_decayment,\n",
    "        percent_punishment=0.8\n",
    "    )\n",
    "    expanded['city_cluster'] = cluster\n",
    "    list_expanded.append(expanded)\n",
    "\n",
    "# concat\n",
    "if len(list_expanded) == 0:\n",
    "    gdf_expanded = pd.DataFrame(columns=gdf_smoothed.columns)\n",
    "else:\n",
    "    gdf_expanded = pd.concat(list_expanded, ignore_index=True)\n",
    "print(f\"Hex expanded: {gdf_expanded.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat with values\n",
    "gdf_terrain_values = pd.concat([gdf_smoothed_with_values, gdf_expanded], ignore_index=True)\n",
    "\n",
    "# see\n",
    "print(gdf_terrain_values.shape)\n",
    "print(hex_gdf.shape)\n",
    "gdf_terrain_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2: Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if there are repeated hexagons\n",
    "print(gdf_terrain_values['hex_id'].duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe stats\n",
    "gdf_terrain_values.loc[:, ['w_mean', 'w_std']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 5 clusters with highest mean terrain value\n",
    "(\n",
    "    gdf_terrain_values\n",
    "    .groupby('city_cluster', as_index=False)\n",
    "    .agg(\n",
    "        mean_w_mean=('w_mean', 'mean'),\n",
    "    )\n",
    "    .set_index('city_cluster')\n",
    "    .sort_values('mean_w_mean', ascending=False)\n",
    "    .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distributions\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "sns.histplot(gdf_terrain_values['w_mean'], bins=50, color='C0', ax=ax)\n",
    "ax.set_title('Weighted Mean Terrain Value Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distributions\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "sns.histplot(gdf_terrain_values['w_std'], bins=50, color='C0', ax=ax)\n",
    "ax.set_title('Weighted Std Terrain Value Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "cols_to_plot = [\n",
    "    'w_mean',\n",
    "    'w_std',\n",
    "    'num_properties'\n",
    "]\n",
    "map_cmap = {\n",
    "    'w_mean': 'viridis',\n",
    "    'w_std': 'magma',\n",
    "    'num_properties': 'bone'\n",
    "}\n",
    "\n",
    "for col in cols_to_plot:\n",
    "    plot_hexagons(\n",
    "        gdf_terrain_values,\n",
    "        col,\n",
    "        cluster=cluster_name,\n",
    "        cmap=map_cmap[col]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpooled\n",
    "(\n",
    "    gdf_unpooled\n",
    "    .query(\"city_cluster == 10\")\n",
    "    .merge(hex_gdf[['hex_id', 'geometry']], on='hex_id', how='left')\n",
    "    .pipe(gpd.GeoDataFrame, crs='EPSG:4326', geometry='geometry')\n",
    "    .explore(\n",
    "        'mean_log_valor_fisico_terreno_m2',\n",
    "        tiles='cartodbpositron',\n",
    "        legend=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothed\n",
    "(\n",
    "    gdf_smoothed\n",
    "    .query(\"city_cluster == 10\")\n",
    "    .merge(hex_gdf[['hex_id', 'geometry']], on='hex_id', how='left')\n",
    "    .pipe(gpd.GeoDataFrame, crs='EPSG:4326', geometry='geometry')\n",
    "    .explore(\n",
    "        'w_mean',\n",
    "        tiles='cartodbpositron',\n",
    "        legend=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# terrain\n",
    "(\n",
    "    gdf_terrain_values\n",
    "    .query(\"city_cluster == 10\")\n",
    "    .merge(hex_gdf[['hex_id', 'geometry']], on='hex_id', how='left')\n",
    "    .pipe(gpd.GeoDataFrame, crs='EPSG:4326', geometry='geometry')\n",
    "    .explore(\n",
    "        'w_mean',\n",
    "        tiles='cartodbpositron',\n",
    "        legend=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save terrain table\n",
    "(\n",
    "    gdf_terrain_values\n",
    "    .drop(columns=['num_properties'])\n",
    "    .rename(columns={\n",
    "        'w_mean': 'mean_log_valor_fisico_terreno_m2',\n",
    "        'w_std': 'std_log_valor_fisico_terreno_m2'\n",
    "    })\n",
    "    .to_parquet(\"../../data/misc/terrain_prices.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# terrain\n",
    "m = (\n",
    "    gdf_terrain_values\n",
    "    .merge(hex_gdf[['hex_id', 'geometry']], on='hex_id', how='left')\n",
    "    .pipe(gpd.GeoDataFrame, crs='EPSG:4326', geometry='geometry')\n",
    "    .explore(\n",
    "        'w_mean',\n",
    "        tiles='cartodbpositron',\n",
    "        legend=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# save map\n",
    "m.save('../../figures/mean_log_terrain_value_ppm2.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# terrain\n",
    "m = (\n",
    "    gdf_terrain_values\n",
    "    .merge(hex_gdf[['hex_id', 'geometry']], on='hex_id', how='left')\n",
    "    .pipe(gpd.GeoDataFrame, crs='EPSG:4326', geometry='geometry')\n",
    "    .explore(\n",
    "        'w_std',\n",
    "        tiles='cartodbpositron',\n",
    "        legend=True,\n",
    "        cmap='magma'\n",
    "    )\n",
    ")\n",
    "\n",
    "# save map\n",
    "m.save('../../figures/std_log_terrain_value_ppm2.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# terrain\n",
    "m = (\n",
    "    gdf_terrain_values\n",
    "    .assign(\n",
    "        exp_mean=lambda x: np.exp(x['w_mean'])\n",
    "    )\n",
    "    .merge(hex_gdf[['hex_id', 'geometry']], on='hex_id', how='left')\n",
    "    .pipe(gpd.GeoDataFrame, crs='EPSG:4326', geometry='geometry')\n",
    "    .explore(\n",
    "        'exp_mean',\n",
    "        tiles='cartodbpositron',\n",
    "        legend=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# save map\n",
    "m.save('../../figures/mean_terrain_value_ppm2.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_terrain_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mds-research-stay",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
